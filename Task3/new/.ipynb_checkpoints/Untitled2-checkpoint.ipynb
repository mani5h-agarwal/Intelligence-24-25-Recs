{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98bc75ea-c463-44f8-bbf8-30a2b0ebd226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF text extraction successful.\n",
      "Structured data saved successfully in 'HogwartsQ&A/prisoner_of_azkaban2.json'.\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Function to structure the data by chapters\n",
    "def structure_chapters(text):\n",
    "    # Enhanced regex pattern to detect \"Chapter <Number>\" and capture titles more reliably\n",
    "    chapter_pattern = r\"(Chapter\\s+\\d+\\s*[:\\-]?.*)\"\n",
    "    \n",
    "    # Split the text based on detected chapter titles using regex\n",
    "    split_text = re.split(chapter_pattern, text)\n",
    "    \n",
    "    # Dictionary to store structured chapter content\n",
    "    structured_data = {}\n",
    "    \n",
    "    # Start at index 1 to skip initial content before Chapter 1, if any\n",
    "    for i in range(1, len(split_text), 2):\n",
    "        chapter_title = split_text[i].strip()  # Chapter title\n",
    "        chapter_content = split_text[i + 1].strip()  # Corresponding content\n",
    "        structured_data[chapter_title] = chapter_content\n",
    "\n",
    "    return structured_data\n",
    "\n",
    "# Main function to run the extraction and structuring\n",
    "def main():\n",
    "    pdf_path = \"3 - Harry Potter and the Prisoner of Azkaban.pdf\"  # Update this path\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Verify extraction success\n",
    "    if extracted_text.strip():\n",
    "        print(\"PDF text extraction successful.\")\n",
    "    else:\n",
    "        print(\"Error: Could not extract text from PDF. Check the file path and content.\")\n",
    "        return\n",
    "    \n",
    "    # Structure the chapters using the extracted text\n",
    "    structured_data = structure_chapters(extracted_text)\n",
    "    \n",
    "    # Save the structured data to a JSON file\n",
    "    with open(\"prisoner_of_azkaban2.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(structured_data, json_file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Structured data saved successfully in 'HogwartsQ&A/prisoner_of_azkaban2.json'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaa1a103-85f7-4a40-bf55-b42e2aa51ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked data saved successfully in 'new/prisoner_of_azkaban_chunked.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Function to split text into chunks with overlap\n",
    "def split_text_into_chunks(text, chunk_size=100, overlap_size=20):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        # Create the chunk and add it to the list\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        # Move the start index for the next chunk\n",
    "        start += chunk_size - overlap_size\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Function to process and chunk each chapter's content\n",
    "def process_and_chunk_chapters(json_path, output_path):\n",
    "    # Load the structured JSON data\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        book_data = json.load(file)\n",
    "\n",
    "    chunked_data = {}\n",
    "    \n",
    "    for chapter, content in book_data.items():\n",
    "        # Split the content into paragraphs based on new lines\n",
    "        paragraphs = content.split(\"\\n\\n\")\n",
    "\n",
    "        # Process each paragraph and split into chunks\n",
    "        chapter_chunks = []\n",
    "        for paragraph in paragraphs:\n",
    "            paragraph = paragraph.strip()  # Remove leading and trailing whitespace\n",
    "            if paragraph:\n",
    "                # Split each paragraph into smaller chunks\n",
    "                paragraph_chunks = split_text_into_chunks(paragraph)\n",
    "                chapter_chunks.extend(paragraph_chunks)\n",
    "\n",
    "        chunked_data[chapter] = chapter_chunks\n",
    "\n",
    "    # Save the chunked data to a new JSON file\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        json.dump(chunked_data, out_file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Chunked data saved successfully in '{output_path}'.\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    json_path = \"prisoner_of_azkaban2.json\"  # Path to the structured JSON file\n",
    "    output_path = \"new/prisoner_of_azkaban_chunked.json\"  # Output path for chunked data\n",
    "    \n",
    "    # Ensure the structured JSON file exists\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"Error: File '{json_path}' not found.\")\n",
    "        return\n",
    "\n",
    "    # Process the chapters and chunk the data\n",
    "    process_and_chunk_chapters(json_path, output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49801b9c-6af1-4117-a383-64f32f62ce49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'all-mpnet-base-v2' loaded successfully.\n",
      "Embeddings and texts saved successfully in 'new/embeddings_new_model.npy' and 'new/embeddings_new_model_texts.json'.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Function to generate embeddings for each text chunk\n",
    "def generate_embeddings_for_chunks(json_path, model_name='all-mpnet-base-v2', output_path=\"HogwartsQ&A/embeddings.npy\"):\n",
    "    # Load the structured chunked data\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        chunked_data = json.load(file)\n",
    "\n",
    "    # Load the pre-trained embedding model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(f\"Model '{model_name}' loaded successfully.\")\n",
    "\n",
    "    # Store all embeddings and corresponding text chunks\n",
    "    all_embeddings = []\n",
    "    all_texts = []\n",
    "    \n",
    "    # Generate embeddings for each chapter's chunks\n",
    "    for chapter, chunks in chunked_data.items():\n",
    "        for chunk in chunks:\n",
    "            embedding = model.encode(chunk)  # Convert the chunk into a dense vector\n",
    "            all_embeddings.append(embedding)\n",
    "            all_texts.append((chapter, chunk))  # Keep track of which chunk belongs to which chapter\n",
    "\n",
    "    # Convert embeddings to a numpy array\n",
    "    all_embeddings = np.array(all_embeddings)\n",
    "\n",
    "    # Save embeddings and corresponding texts\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    np.save(output_path, all_embeddings)\n",
    "    \n",
    "    # Save text references for easy retrieval\n",
    "    with open(output_path.replace(\".npy\", \"_texts.json\"), \"w\", encoding=\"utf-8\") as out_file:\n",
    "        json.dump(all_texts, out_file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    print(f\"Embeddings and texts saved successfully in '{output_path}' and '{output_path.replace('.npy', '_texts.json')}'.\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    chunked_json_path = \"new/prisoner_of_azkaban_chunked.json\"  # Path to the chunked data JSON file\n",
    "    embeddings_output_path = \"new/embeddings_new_model.npy\"  # Output path for the embeddings\n",
    "    \n",
    "    # Ensure the chunked JSON file exists\n",
    "    if not os.path.exists(chunked_json_path):\n",
    "        print(f\"Error: File '{chunked_json_path}' not found. Please run the chunking step first.\")\n",
    "        return\n",
    "\n",
    "    # Generate and save embeddings\n",
    "    generate_embeddings_for_chunks(chunked_json_path, output_path=embeddings_output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1770842-f392-4475-a680-49f6dbe962ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings stored in FAISS index successfully at 'new/faiss_index.index'.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Function to store embeddings in FAISS\n",
    "def store_embeddings_in_faiss(embeddings_path, texts_path, index_path=\"new/faiss_index.index\"):\n",
    "    # Load embeddings and corresponding texts\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    with open(texts_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        texts = json.load(file)\n",
    "\n",
    "    # Create a FAISS index\n",
    "    dimension = embeddings.shape[1]  # Number of dimensions of embeddings\n",
    "    index = faiss.IndexFlatL2(dimension)  # Using L2 distance for similarity search\n",
    "\n",
    "    # Add embeddings to the index\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # Save the index to disk\n",
    "    faiss.write_index(index, index_path)\n",
    "\n",
    "    # Save texts for retrieval\n",
    "    with open(index_path.replace(\".index\", \"_texts.json\"), \"w\", encoding=\"utf-8\") as out_file:\n",
    "        json.dump(texts, out_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Embeddings stored in FAISS index successfully at '{index_path}'.\")\n",
    "\n",
    "# Main function to run the FAISS embedding storage\n",
    "def main():\n",
    "    chunked_json_path = \"new/prisoner_of_azkaban_chunked.json\"\n",
    "    embeddings_output_path = \"new/embeddings_new_model.npy\"\n",
    "    texts_output_path = \"new/embeddings_new_model_texts.json\"  # Corresponding texts\n",
    "\n",
    "    # Ensure the embeddings and texts files exist\n",
    "    if not os.path.exists(embeddings_output_path) or not os.path.exists(texts_output_path):\n",
    "        print(\"Error: Embeddings or texts files not found. Please run the embedding generation step first.\")\n",
    "        return\n",
    "\n",
    "    # Store embeddings in FAISS\n",
    "    store_embeddings_in_faiss(embeddings_output_path, texts_output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4feca539-a097-4eb2-8472-6307e8257557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  who is harry potter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top relevant chunks:\n",
      "Chunk: ['Chapter 4\\nThe Leaky Cauldron', 'a round-faced, forgetful boy, outside Flourish and Blotts. Harry didn’t stop to chat; Neville appeared to have mislaid his booklist and was being told oﬀ by his very formidable-looking grandmother. Harry hoped she never found out that he’d pretended to be Neville while on the run from the Ministry of Magic. Harry woke on the last day of the holidays, thinking that he would at least meet Ron and Hermione tomorrow, on the Hogwarts Express. He got up, dressed, went for a last look at the Firebolt, and was just wondering where he’d have lunch, when someone yelled his name'], Distance: 0.922834575176239\n",
      "\n",
      "Chunk: ['Chapter 1\\nOwl Post', 'Harry Potter was a highly unusual boy in many ways. For one thing, he hated the summer holidays more than any other time of year. For another, he really wanted to do his homework but was forced to do it in secret, in the dead of night. And he also happened to be a wizard. It was nearly midnight, and he was lying on his stomach in bed, the blankets drawn right over his head like a tent, a ﬂashlight in one hand and a large leather-bound book (A History of Magic by Bathilda Bagshot) propped open against the pillow.'], Distance: 0.9562285542488098\n",
      "\n",
      "Chunk: ['Chapter 4\\nThe Leaky Cauldron', 'wizards until he glimpsed a newly erected podium, on which was mounted the most magniﬁcent broom he had ever seen in his life. “Just come out — prototype —” a square-jawed wizard was telling his companion. “It’s the fastest broom in the world, isn’t it, Dad?” squeaked a boy younger than Harry, who was swinging oﬀ his father’s arm. “Irish International Side’s just put in an order for seven of these beauties!” the proprietor of the shop told the crowd. “And they’re favorites for the World Cup!” A large witch in front of Harry moved, and he was able to'], Distance: 0.9903185367584229\n",
      "\n",
      "Chunk: ['Chapter 12\\nThe Patronus', 'magic, Harry — well beyond Ordinary Wizarding Level. It is called the Patronus Charm.” “How does it work?” said Harry nervously. “Well, when it works correctly, it conjures up a Patronus,” said Lupin, “which is a kind of anti-dementor — a guardian that acts as a shield between you and the dementor.” Harry had a sudden vision of himself crouching behind a Hagrid-sized ﬁgure holding a large club. Professor Lupin continued, “The Patronus is a kind of positive force, a projection of the very things that the dementor feeds upon — hope, happiness, the desire to survive — but it'], Distance: 1.040748953819275\n",
      "\n",
      "Chunk: ['Chapter 4\\nThe Leaky Cauldron', 'child. He’s thirteen years old and —” “Arthur, the truth would terrify him!” said Mrs. Weasley shrilly. “Do you really want to send Harry back to school with that hanging over him? For heaven’s sake, he’s happy not knowing!” “I don’t want to make him miserable, I want to put him on his guard!” retorted Mr. Weasley. “You know what Harry and Ron are like, wandering oﬀ by themselves — they’ve even ended up in the Forbidden Forest! But Harry mustn’t do that this year! When I think what could have happened to him that night he ran away from'], Distance: 1.041727066040039\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Function to query FAISS\n",
    "def query_faiss(user_query, model_name='all-mpnet-base-v2', top_n=5, index_path=\"new/faiss_index.index\"):\n",
    "    # Load the pre-trained embedding model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Convert user query to embedding\n",
    "    query_embedding = model.encode(user_query)\n",
    "\n",
    "    # Load the FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Perform a similarity search\n",
    "    D, I = index.search(np.array([query_embedding]), top_n)  # D: distances, I: indices of nearest neighbors\n",
    "\n",
    "    # Load the corresponding texts\n",
    "    texts_path = index_path.replace(\".index\", \"_texts.json\")\n",
    "    with open(texts_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        texts = json.load(file)\n",
    "\n",
    "    # Retrieve top N relevant chunks\n",
    "    relevant_chunks = [(texts[i], D[0][idx]) for idx, i in enumerate(I[0])]\n",
    "\n",
    "    return relevant_chunks\n",
    "\n",
    "# Main function to run the query handling\n",
    "def main():\n",
    "    user_query = input(\"Enter your query: \")\n",
    "    top_n = 5  # Number of top results to retrieve\n",
    "\n",
    "    results = query_faiss(user_query, top_n=top_n)\n",
    "    print(\"Top relevant chunks:\")\n",
    "    for chunk, distance in results:\n",
    "        print(f\"Chunk: {chunk}, Distance: {distance}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04226ed4-3d99-4221-a30b-fecf35596efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'all-mpnet-base-v2' loaded successfully.\n",
      "FAISS index loaded successfully.\n",
      "LLM Generated Response:\n",
      " Cho Chang\n",
      "Related Questions: harry potter girlfriend name\n",
      "\" Someone who comes from a particularly good wizarding family — Can't say I've gotten to know her very well, but she seems to be part of that set that fawns over the Malfoys.\"\n",
      "His new girlfriend, Cheryl Rodewald. Interestingly, Granger, now known professionally as Hermione, Another Weekend. He asked various relationships Hermione for her help, stressing that he believed in her, Ginny. Toonkey\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "\n",
    "# Define your Hugging Face API token and model endpoint\n",
    "# API_TOKEN = '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'\n",
    "# MODEL_NAME = 'openai-community/gpt2'  # Replace with the desired Hugging Face model\n",
    "\n",
    "API_TOKEN = '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'\n",
    "MODEL_NAME = 'meta-llama/Llama-3.2-11B-Vision-Instruct'  # Replace with the desired Hugging Face model\n",
    "# Function to generate a response from the LLM using the Hugging Face API\n",
    "def generate_response(prompt):\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {API_TOKEN}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    payload = {\n",
    "        'inputs': prompt,\n",
    "        'options': {\n",
    "            'use_cache': False,  # Disable caching to get real-time responses\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Make the POST request to the Hugging Face model endpoint\n",
    "    response = requests.post(f'https://api-inference.huggingface.co/models/{MODEL_NAME}', headers=headers, json=payload)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            # Attempt to extract and return the generated text from the response\n",
    "            return response.json()[0]['generated_text']\n",
    "        except (KeyError, IndexError) as e:\n",
    "            print(\"Unexpected response structure:\", response.json(), \"Error:\", e)\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code, response.text)\n",
    "        return None\n",
    "\n",
    "# Function to load embeddings and text data\n",
    "def load_embeddings_and_texts(embeddings_path, texts_path):\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    with open(texts_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        texts = json.load(file)\n",
    "    return embeddings, texts\n",
    "\n",
    "# Function to load the FAISS index from disk\n",
    "def load_faiss_index(index_path):\n",
    "    index = faiss.read_index(index_path)\n",
    "    return index\n",
    "\n",
    "# Function to handle user queries and retrieve relevant chunks\n",
    "def handle_query(query, model, index, texts, top_k=5):\n",
    "    # Convert the user query into an embedding\n",
    "    query_embedding = model.encode(query)\n",
    "    \n",
    "    # Use the FAISS index to find top-N similar chunks\n",
    "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
    "    \n",
    "    # Retrieve the corresponding text chunks based on the indices\n",
    "    results = []\n",
    "    for idx, dist in zip(indices[0], distances[0]):\n",
    "        chapter, chunk = texts[idx]  # Get chapter and text chunk based on index\n",
    "        results.append({\n",
    "            \"chapter\": chapter,\n",
    "            \"text_chunk\": chunk,\n",
    "            \"distance\": dist\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to create a cohesive prompt using top retrieved chunks\n",
    "def create_prompt(query, top_chunks):\n",
    "    # Combine the top k chunks as context for the generative model\n",
    "    context = \" \".join([chunk[\"text_chunk\"] for chunk in top_chunks])\n",
    "    \n",
    "    # Create a structured prompt with context and question\n",
    "    prompt = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "# Main function to handle the query, retrieve top chunks, and generate a response\n",
    "def main():\n",
    "    # Paths to the saved files (assuming you have these paths set up)\n",
    "    embeddings_path = \"new/embeddings_new_model.npy\"\n",
    "    texts_path = \"new/embeddings_new_model_texts.json\"\n",
    "    faiss_index_path = \"new/faiss_index.index\"\n",
    "\n",
    "    # Load the pre-trained embedding model\n",
    "    sentence_model_name = 'all-mpnet-base-v2'\n",
    "    sentence_model = SentenceTransformer(sentence_model_name)\n",
    "    print(f\"Model '{sentence_model_name}' loaded successfully.\")\n",
    "    \n",
    "    # Load embeddings and text references\n",
    "    embeddings, texts = load_embeddings_and_texts(embeddings_path, texts_path)\n",
    "\n",
    "    # Load the FAISS index\n",
    "    index = load_faiss_index(faiss_index_path)\n",
    "    print(\"FAISS index loaded successfully.\")\n",
    "    \n",
    "    # Example query\n",
    "    user_query = \"who is harry potter's girlfriend\"\n",
    "\n",
    "    # Handle the user query and retrieve top 5 results\n",
    "    top_results = handle_query(user_query, sentence_model, index, texts, top_k=5)\n",
    "\n",
    "    # Display the top-5 chunks retrieved\n",
    "\n",
    "    # Create a cohesive prompt using the top retrieved chunks\n",
    "    final_prompt = create_prompt(user_query, top_results)\n",
    "\n",
    "    # Generate a response using the Hugging Face API and the final prompt\n",
    "    generated_answer = generate_response(final_prompt)\n",
    "\n",
    "    # Display the final answer\n",
    "    print(\"LLM Generated Response:\")\n",
    "    if generated_answer:\n",
    "        print(generated_answer)\n",
    "    else:\n",
    "        print(\"No valid response received from the model.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3c7bd63-8dc1-45ca-a792-cfe48719ba51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [16872]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59003 - \"POST /generate-answer HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59066 - \"POST /generate-answer HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59109 - \"POST /generate-answer HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59109 - \"POST /generate-answer HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59189 - \"POST /generate-answer HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59189 - \"POST /generate-answer HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59189 - \"POST /generate-answer HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59189 - \"POST /generate-answer HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59279 - \"POST /generate-answer HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [16872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Application is shutting down.\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import requests\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "import uvicorn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from contextlib import asynccontextmanager\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import HTMLResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Global configuration variables\n",
    "API_TOKEN = os.getenv(\"HUGGING_FACE_API_TOKEN\", \"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\")  # Replace with your API Token or set as an environment variable\n",
    "MODEL_NAME = os.getenv(\"HUGGING_FACE_MODEL_NAME\", \"meta-llama/Llama-3.2-11B-Vision-Instruct\")  # Replace with model name or set as an environment variable\n",
    "SENTENCE_MODEL_NAME = 'all-mpnet-base-v2'\n",
    "\n",
    "EMBEDDINGS_PATH = \"embeddings_new_model.npy\"\n",
    "TEXTS_PATH = \"embeddings_new_model_texts.json\"\n",
    "FAISS_INDEX_PATH = \"faiss_index.index\"\n",
    "\n",
    "# Load pre-trained model and FAISS index\n",
    "sentence_model = SentenceTransformer(SENTENCE_MODEL_NAME)\n",
    "index = None\n",
    "texts = None\n",
    "\n",
    "def load_embeddings_and_texts(embeddings_path, texts_path):\n",
    "    try:\n",
    "        with open(texts_path, \"r\") as f:\n",
    "            texts = json.load(f)\n",
    "        embeddings = np.load(embeddings_path)\n",
    "        return embeddings, texts\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading embeddings or texts: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to load the FAISS index\n",
    "def load_faiss_index(faiss_index_path):\n",
    "    try:\n",
    "        if not os.path.exists(faiss_index_path):\n",
    "            raise FileNotFoundError(f\"FAISS index file does not exist at {faiss_index_path}\")\n",
    "        index = faiss.read_index(faiss_index_path)\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FAISS index: {e}\")\n",
    "        raise\n",
    "\n",
    "# Use an async context manager to handle startup and shutdown\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    global index, texts\n",
    "    try:\n",
    "        embeddings, texts = load_embeddings_and_texts(EMBEDDINGS_PATH, TEXTS_PATH)\n",
    "        index = load_faiss_index(FAISS_INDEX_PATH)\n",
    "        if index is None:\n",
    "            raise ValueError(\"Failed to load FAISS index.\")\n",
    "        yield  # The application will run while this context is active\n",
    "    finally:\n",
    "        print(\"Application is shutting down.\")\n",
    "\n",
    "# Define the FastAPI app\n",
    "app = FastAPI(lifespan=lifespan)\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Specify your frontend origin\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n",
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def serve_html():\n",
    "    html_path = \"static/chat-ui.html\"\n",
    "    with open(html_path, \"r\") as html_file:\n",
    "        return HTMLResponse(content=html_file.read(), status_code=200)\n",
    "\n",
    "# Define the request and response models\n",
    "class QueryRequest(BaseModel):\n",
    "    user_query: str\n",
    "\n",
    "class ResponseModel(BaseModel):\n",
    "    generated_answer: str\n",
    "\n",
    "# Function to handle query and retrieve relevant chunks\n",
    "def handle_query(query, model, index, texts, top_k=5):\n",
    "    query_embedding = model.encode(query)\n",
    "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
    "    \n",
    "    results = []\n",
    "    for idx, dist in zip(indices[0], distances[0]):\n",
    "        chapter, chunk = texts[idx]\n",
    "        results.append({\n",
    "            \"chapter\": chapter,\n",
    "            \"text_chunk\": chunk,\n",
    "            \"distance\": dist\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to generate a response from the LLM using the Hugging Face API\n",
    "def generate_response(prompt, max_new_tokens=50):\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {API_TOKEN}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    payload = {\n",
    "        'inputs': prompt,\n",
    "        'options': {\n",
    "            'use_cache': False,\n",
    "            'max_new_tokens': max_new_tokens\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(f'https://api-inference.huggingface.co/models/{MODEL_NAME}', headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            return response.json()[0]['generated_text']\n",
    "        except (KeyError, IndexError) as e:\n",
    "            print(\"Unexpected response structure:\", response.json(), \"Error:\", e)\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code, response.text)\n",
    "        return None\n",
    "\n",
    "# Function to create a cohesive prompt using top retrieved chunks\n",
    "def create_prompt(query, top_chunks, max_tokens=900):\n",
    "    combined_context = \"\"\n",
    "    current_token_count = 0\n",
    "\n",
    "    for chunk in top_chunks:\n",
    "        chunk_text = chunk[\"text_chunk\"]\n",
    "        chunk_token_count = len(chunk_text) // 5\n",
    "\n",
    "        if current_token_count + chunk_token_count <= max_tokens:\n",
    "            combined_context += f\"{chunk_text} \"\n",
    "            current_token_count += chunk_token_count\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    prompt = f\"Context: {combined_context.strip()}\\nQuestion: {query}\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "# Define the main endpoint for handling query-response interactions\n",
    "# @app.post(\"/generate-answer\", response_model=ResponseModel)\n",
    "# async def generate_answer(query_request: QueryRequest):\n",
    "#     user_query = query_request.user_query\n",
    "\n",
    "#     # Retrieve the top 5 chunks from the FAISS index\n",
    "#     top_results = handle_query(user_query, sentence_model, index, texts, top_k=5)\n",
    "    \n",
    "#     if not top_results:\n",
    "#         raise HTTPException(status_code=404, detail=\"No relevant chunks found.\")\n",
    "\n",
    "#     # Create a prompt with the top chunks\n",
    "#     final_prompt = create_prompt(user_query, top_results, max_tokens=900)\n",
    "\n",
    "#     # Generate a response using the Hugging Face API\n",
    "#     generated_answer = generate_response(final_prompt, max_new_tokens=50)\n",
    "    \n",
    "#     if not generated_answer:\n",
    "#         raise HTTPException(status_code=500, detail=\"Failed to generate a response from the model.\")\n",
    "\n",
    "#     return {\"generated_answer\": generated_answer}\n",
    "\n",
    "\n",
    "@app.post(\"/generate-answer\", response_model=ResponseModel)\n",
    "async def generate_answer(query_request: QueryRequest):\n",
    "    user_query = query_request.user_query\n",
    "\n",
    "    # Retrieve the top 5 chunks from the FAISS index\n",
    "    top_results = handle_query(user_query, sentence_model, index, texts, top_k=5)\n",
    "    \n",
    "    if not top_results:\n",
    "        raise HTTPException(status_code=404, detail=\"No relevant chunks found.\")\n",
    "\n",
    "    # Create a prompt with the top chunks\n",
    "    final_prompt = create_prompt(user_query, top_results, max_tokens=900)\n",
    "\n",
    "    # Generate a response using the Hugging Face API\n",
    "    generated_answer = generate_response(final_prompt, max_new_tokens=50)\n",
    "    \n",
    "    if not generated_answer:\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to generate a response from the model.\")\n",
    "\n",
    "    # Find the last full stop in the generated answer\n",
    "    last_full_stop_idx = generated_answer.rfind('.')\n",
    "    \n",
    "    if last_full_stop_idx != -1:\n",
    "        # If a full stop exists, truncate the answer up to the last full stop\n",
    "        truncated_answer = generated_answer[:last_full_stop_idx + 1]\n",
    "    else:\n",
    "        # If no full stop is found, return the answer as it is\n",
    "        truncated_answer = generated_answer\n",
    "\n",
    "    return {\"generated_answer\": truncated_answer}\n",
    "\n",
    "\n",
    "# Run the FastAPI app\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574afac-f941-49ca-88dd-eaca35146ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import requests\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "import uvicorn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from contextlib import asynccontextmanager\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import HTMLResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Global configuration variables\n",
    "API_TOKEN = os.getenv(\"HUGGING_FACE_API_TOKEN\", \"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\")  # Replace with your API Token or set as an environment variable\n",
    "MODEL_NAME = os.getenv(\"HUGGING_FACE_MODEL_NAME\", \"meta-llama/Llama-3.2-11B-Vision-Instruct\")  # Replace with model name or set as an environment variable\n",
    "SENTENCE_MODEL_NAME = 'all-mpnet-base-v2'\n",
    "\n",
    "EMBEDDINGS_PATH = \"embeddings_new_model.npy\"\n",
    "TEXTS_PATH = \"embeddings_new_model_texts.json\"\n",
    "FAISS_INDEX_PATH = \"faiss_index.index\"\n",
    "\n",
    "# Load pre-trained model and FAISS index\n",
    "sentence_model = SentenceTransformer(SENTENCE_MODEL_NAME)\n",
    "index = None\n",
    "texts = None\n",
    "\n",
    "def load_embeddings_and_texts(embeddings_path, texts_path):\n",
    "    try:\n",
    "        with open(texts_path, \"r\") as f:\n",
    "            texts = json.load(f)\n",
    "        embeddings = np.load(embeddings_path)\n",
    "        return embeddings, texts\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading embeddings or texts: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to load the FAISS index\n",
    "def load_faiss_index(faiss_index_path):\n",
    "    try:\n",
    "        if not os.path.exists(faiss_index_path):\n",
    "            raise FileNotFoundError(f\"FAISS index file does not exist at {faiss_index_path}\")\n",
    "        index = faiss.read_index(faiss_index_path)\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FAISS index: {e}\")\n",
    "        raise\n",
    "\n",
    "# Use an async context manager to handle startup and shutdown\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    global index, texts\n",
    "    try:\n",
    "        embeddings, texts = load_embeddings_and_texts(EMBEDDINGS_PATH, TEXTS_PATH)\n",
    "        index = load_faiss_index(FAISS_INDEX_PATH)\n",
    "        if index is None:\n",
    "            raise ValueError(\"Failed to load FAISS index.\")\n",
    "        yield  # The application will run while this context is active\n",
    "    finally:\n",
    "        print(\"Application is shutting down.\")\n",
    "\n",
    "# Define the FastAPI app\n",
    "app = FastAPI(lifespan=lifespan)\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Specify your frontend origin\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n",
    "\n",
    "# Define the request and response models\n",
    "class QueryRequest(BaseModel):\n",
    "    user_query: str\n",
    "\n",
    "class ResponseModel(BaseModel):\n",
    "    generated_answer: str\n",
    "\n",
    "# Function to handle query and retrieve relevant chunks\n",
    "def handle_query(query, model, index, texts, top_k=5):\n",
    "    query_embedding = model.encode(query)\n",
    "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
    "    \n",
    "    results = []\n",
    "    for idx, dist in zip(indices[0], distances[0]):\n",
    "        chapter, chunk = texts[idx]\n",
    "        results.append({\n",
    "            \"chapter\": chapter,\n",
    "            \"text_chunk\": chunk,\n",
    "            \"distance\": dist\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to generate a response from the LLM using the Hugging Face API\n",
    "def generate_response(prompt, max_new_tokens=50):\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {API_TOKEN}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    payload = {\n",
    "        'inputs': prompt,\n",
    "        'options': {\n",
    "            'use_cache': False,\n",
    "            'max_new_tokens': max_new_tokens\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(f'https://api-inference.huggingface.co/models/{MODEL_NAME}', headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            return response.json()[0]['generated_text']\n",
    "        except (KeyError, IndexError) as e:\n",
    "            print(\"Unexpected response structure:\", response.json(), \"Error:\", e)\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code, response.text)\n",
    "        return None\n",
    "\n",
    "# Function to create a cohesive prompt using top retrieved chunks\n",
    "def create_prompt(query, top_chunks, max_tokens=900):\n",
    "    combined_context = \"\"\n",
    "    current_token_count = 0\n",
    "\n",
    "    for chunk in top_chunks:\n",
    "        chunk_text = chunk[\"text_chunk\"]\n",
    "        chunk_token_count = len(chunk_text) // 5\n",
    "\n",
    "        if current_token_count + chunk_token_count <= max_tokens:\n",
    "            combined_context += f\"{chunk_text} \"\n",
    "            current_token_count += chunk_token_count\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Constructing the prompt to request an answer with reasoning\n",
    "    prompt = (\n",
    "        f\"Context: {combined_context.strip()}\\n\"\n",
    "        f\"Question: {query}\\n\"\n",
    "        f\"Please provide a detailed answer with reasoning and implications:\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# Define the main endpoint for handling query-response interactions\n",
    "@app.post(\"/generate-answer\", response_model=ResponseModel)\n",
    "async def generate_answer(query_request: QueryRequest):\n",
    "    user_query = query_request.user_query\n",
    "\n",
    "    # Retrieve the top 5 chunks from the FAISS index\n",
    "    top_results = handle_query(user_query, sentence_model, index, texts, top_k=5)\n",
    "    \n",
    "    if not top_results:\n",
    "        raise HTTPException(status_code=404, detail=\"No relevant chunks found.\")\n",
    "\n",
    "    # Create a prompt with the top chunks\n",
    "    final_prompt = create_prompt(user_query, top_results, max_tokens=900)\n",
    "\n",
    "    # Generate a response using the Hugging Face API\n",
    "    generated_answer = generate_response(final_prompt, max_new_tokens=100)\n",
    "    \n",
    "    if not generated_answer:\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to generate a response from the model.\")\n",
    "\n",
    "    # Post-processing logic to enhance the answer's structure\n",
    "    if len(generated_answer.split()) < 15:  # Check if the answer is too short\n",
    "        generated_answer += \" Please elaborate on your answer.\"\n",
    "\n",
    "    # Find the last full stop in the generated answer for truncation\n",
    "    last_full_stop_idx = generated_answer.rfind('.')\n",
    "    \n",
    "    if last_full_stop_idx != -1:\n",
    "        # If a full stop exists, truncate the answer up to the last full stop\n",
    "        truncated_answer = generated_answer[:last_full_stop_idx + 1]\n",
    "    else:\n",
    "        # If no full stop is found, return the answer as it is\n",
    "        truncated_answer = generated_answer\n",
    "\n",
    "    return {\"generated_answer\": truncated_answer}\n",
    "\n",
    "# Run the FastAPI app\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea574d-769b-4d81-9a12-a445f5842813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
